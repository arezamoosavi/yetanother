version: "3"

services:
  spark: &basePy
    build:
      context: .
      dockerfile: ./docker-py/Dockerfile
    ports:
      - "8888:8888" #jupyter
      - "8080:8080" #spark
      - "4040:4040" #spark job
      - "4041:4041" #spark job
      - "4042:4042" #spark job
      - "4043:4043" #spark job
    volumes:
      - ./dags:/app/dags
    restart: unless-stopped
    command: sh /jupyter_run.sh

  airflow:
    <<: *basePy
    restart: unless-stopped
    environment:
      - AIRFLOW_HOME=/app
    ports:
      - "8000:8000"
    command: sh /airflow_run.sh
